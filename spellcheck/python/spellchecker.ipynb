{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from collections import Counter\n",
    "import numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words(text): return re.findall(r'\\w+', text.lower())\n",
    "\n",
    "#WORDS = Counter(words(open('coba.rtf').read()))\n",
    "\n",
    "def load_book(path):\n",
    "    input_file = os.path.join(path)\n",
    "    with open(input_file) as f:\n",
    "        book = f.read()\n",
    "    return book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './books/'\n",
    "book_files = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "book_files = book_files[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "There are 354531 words in Ilana Tan - Autumn in Paris sad.rtf.\n",
      "There are 297184 words in Ilana Tan - Summer in Seoul.rtf.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'{\\\\rtf1\\\\adeflang1025\\\\ansi\\\\ansicpg1252\\\\uc1\\\\adeff31507\\\\deff0\\\\stshfdbch31506\\\\stshfloch31506\\\\stshfhich31506\\\\stshfbi31507\\\\deflang3079\\\\deflangfe3079\\\\themelang3079\\\\themelangfe0\\\\themelangcs0{\\\\fonttbl{\\\\f0\\\\fbidi \\\\froman\\\\fcharset0\\\\fprq2{\\\\*\\\\panose 02020603050405020304}Times New Roman;}{\\\\f1\\\\fbidi \\\\fswiss\\\\fcharset0\\\\fprq2{\\\\*\\\\panose 020b0604020202020204}Arial;}\\n{\\\\f2\\\\fbidi \\\\fmodern\\\\fcharset0\\\\fprq1{\\\\*\\\\panose 02070309020205020404}Courier New;}{\\\\f3\\\\fbidi \\\\froman\\\\fcharset2\\\\fprq2{\\\\*\\\\panose 05050102010706020507}Symbol;'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "books = []\n",
    "for book in book_files:\n",
    "    books.append(load_book(path+book))\n",
    "print(len(books))\n",
    "for i in range(len(books)):\n",
    "    print(\"There are {} words in {}.\".format(len(books[i].split()), book_files[i]))\n",
    "\n",
    "books[0][:500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    '''Remove unwanted characters and extra spaces from the text'''\n",
    "    text = re.sub(r'\\n', ' ', text) \n",
    "    text = re.sub(r'[{}@_*>()\\\\#%+=\\[\\]]','', text)\n",
    "    text = re.sub('a0','', text)\n",
    "    text = re.sub('\\'92t','\\'t', text)\n",
    "    text = re.sub('\\'92s','\\'s', text)\n",
    "    text = re.sub('\\'92m','\\'m', text)\n",
    "    text = re.sub('\\'92ll','\\'ll', text)\n",
    "    text = re.sub('\\'91','', text)\n",
    "    text = re.sub('\\'92','', text)\n",
    "    text = re.sub('\\'93','', text)\n",
    "    text = re.sub('\\'94','', text)\n",
    "    text = re.sub('\\.','. ', text)\n",
    "    text = re.sub('\\!','! ', text)\n",
    "    text = re.sub('\\?','? ', text)\n",
    "    text = re.sub(' +',' ', text)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_books = []\n",
    "for book in books:\n",
    "    clean_books.append(clean_text(book))\n",
    "\n",
    "clean_books[0][:500]\n",
    "\n",
    "vocab_to_int = {}\n",
    "count = 0\n",
    "for book in clean_books:\n",
    "    for character in book:\n",
    "        if character not in vocab_to_int:\n",
    "            vocab_to_int[character] = count\n",
    "            count += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7716 sentences.\n"
     ]
    }
   ],
   "source": [
    "sentences = []\n",
    "for book in clean_books:\n",
    "    for sentence in book.split('. '):\n",
    "        sentences.append(sentence + '.')\n",
    "print(\"There are {} sentences.\".format(len(sentences)))\n",
    "\n",
    "Tokens = []\n",
    "for each_line in sentences:\n",
    "    each_line = re.sub('[.]','', each_line)\n",
    "    token = each_line.split()\n",
    "\n",
    "    for each_word in token:\n",
    "    \tTokens.append(each_word)\n",
    "\n",
    "WORDS = Counter(Tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def P(word, N=sum(WORDS.values())): \n",
    "     # \"Probability of `word`.\"\n",
    "     return WORDS[word] / N\n",
    "\n",
    "def correction(word):\n",
    "    # \"Most probable spelling correction for word.\"\n",
    "    return max(candidates(word), key=P)\n",
    "\n",
    "def candidates(word): \n",
    "    # \"Generate possible spelling corrections for word.\"\n",
    "    return (known([word]) or known(edits1(word)) or known(edits2(word)) or [word])\n",
    "\n",
    "def known(words): \n",
    "    # \"The subset of `words` that appear in the dictionary of WORDS.\"\n",
    "    return set(w for w in words if w in WORDS)\n",
    "\n",
    "def edits1(word):\n",
    "    # \"All edits that are one edit away from `word`.\"\n",
    "    letters    = 'abcdefghijklmnopqrstuvwxyz'\n",
    "    splits     = [(word[:i], word[i:])    for i in range(len(word) + 1)] # [('', 'kemarin'), ('k', 'emarin'), ('ke', 'marin'), dst]\n",
    "    deletes    = [L + R[1:]               for L, R in splits if R] # ['emarin', 'kmarin', 'kearin', dst]\n",
    "    transposes = [L + R[1] + R[0] + R[2:] for L, R in splits if len(R)>1] # ['ekmarin', 'kmearin', 'keamrin', dst]\n",
    "    replaces   = [L + c + R[1:]           for L, R in splits if R for c in letters] # ['aemarin', 'bemarin', 'cemarin', dst]\n",
    "    inserts    = [L + c + R               for L, R in splits for c in letters] # ['akemarin', 'bkemarin', 'ckemarin', dst]\n",
    "    return set(deletes + transposes + replaces + inserts)\n",
    "\n",
    "def edits2(word): \n",
    "    # \"All edits that are two edits away from `word`.\"\n",
    "    return (e2 for e1 in edits1(word) for e2 in edits1(e1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testSpellCheck(word):\n",
    "    return candidates(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'malam', 'malu'}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testSpellCheck(\"maklum\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
