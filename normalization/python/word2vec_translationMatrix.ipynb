{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import common_texts\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "model = Word2Vec.load('./models/idwiki_word2vec.model')\n",
    "modelOOV = Word2Vec.load('./models/modelWord2Vec_OOV.model')\n",
    "word_vectors = model.wv\n",
    "word_vectorsOOV = modelOOV.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.test.utils import get_tmpfile\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "fname = get_tmpfile(\"vectors.kv\")\n",
    "fnameOOV = get_tmpfile(\"vectorsOOV.kv\")\n",
    "\n",
    "word_vectors.save(fname)\n",
    "word_vectorsOOV.save(fnameOOV)\n",
    "\n",
    "word_vectors = KeyedVectors.load(fname, mmap='r')\n",
    "word_vectorsOOV = KeyedVectors.load(fnameOOV, mmap='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training pairs...\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import codecs\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "\n",
    "print('Reading training pairs...')\n",
    "word_pairs = codecs.open('corpus_TBK-BK.csv', 'r', 'utf-8')\n",
    "\n",
    "pairs = pd.read_csv(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          source         target\n",
      "0          abjat          abjad\n",
      "1             da            ada\n",
      "2             ad            ada\n",
      "3           adek           adik\n",
      "4        adpokat        advokat\n",
      "5          afdol          afdal\n",
      "6       agamawan     ahli agama\n",
      "7              z            aja\n",
      "8            ajk           ajak\n",
      "9         ajakin          ajaki\n",
      "10         ahlak         akhlak\n",
      "11         aktip          aktif\n",
      "12     aktifitas      aktivitas\n",
      "13           ane            aku\n",
      "14            aq            aku\n",
      "15           aqu            aku\n",
      "16           eke            aku\n",
      "17          eyke            aku\n",
      "18           gua            aku\n",
      "19            ak            aku\n",
      "20             q            aku\n",
      "21          almt         alamat\n",
      "22           alm       almarhum\n",
      "23       ambeyen        ambeien\n",
      "24       ambulan       ambulans\n",
      "25     amandemen      amendemen\n",
      "26       amphibi         amfibi\n",
      "27         amvun          ampun\n",
      "28           ank           anak\n",
      "29          alay  anak layangan\n",
      "...          ...            ...\n",
      "1047        aduh          waduh\n",
      "1048        walo          walau\n",
      "1049     walopun       walaupun\n",
      "1050        wlpn       walaupun\n",
      "1051    walikota      wali kota\n",
      "1052         wnt         wanita\n",
      "1053         wrn          warna\n",
      "1054        wrna          warna\n",
      "1055       wasap       whatsapp\n",
      "1056   wirausaha     wira usaha\n",
      "1057       wudhu           wudu\n",
      "1058         yaa             ya\n",
      "1059         yap             ya\n",
      "1060         yha             ya\n",
      "1061         yoi             ya\n",
      "1062         yup             ya\n",
      "1063        yups             ya\n",
      "1064           y             ya\n",
      "1065          yg           yang\n",
      "1066         yng           yang\n",
      "1067      yaudah        yasudah\n",
      "1068       yutub        youtube\n",
      "1069       yutup        youtube\n",
      "1070   judikatif      yudikatif\n",
      "1071    judisial       yudisial\n",
      "1072  jurisdiksi     yurisdiksi\n",
      "1073       jaman          zaman\n",
      "1074     zam-zam         zamzam\n",
      "1075       jinah           zina\n",
      "1076        zone           zona\n",
      "\n",
      "[1077 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "doc2bow expects an array of unicode tokens on input, not a single string",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-452f0a404a37>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mgensim\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mdictionary\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcorpora\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDictionary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpairs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m     79\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdocuments\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 81\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_documents\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocuments\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprune_at\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mprune_at\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     82\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtokenid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36madd_documents\u001b[1;34m(self, documents, prune_at)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    197\u001b[0m             \u001b[1;31m# update Dictionary with the document\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 198\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoc2bow\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_update\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# ignore the result, here we only care about updating token ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    199\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m         logger.info(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\gensim\\corpora\\dictionary.py\u001b[0m in \u001b[0;36mdoc2bow\u001b[1;34m(self, document, allow_update, return_missing)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \"\"\"\n\u001b[0;32m    235\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdocument\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 236\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"doc2bow expects an array of unicode tokens on input, not a single string\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    237\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m         \u001b[1;31m# Construct (word, frequency) mapping.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: doc2bow expects an array of unicode tokens on input, not a single string"
     ]
    }
   ],
   "source": [
    "\n",
    "from gensim import corpora\n",
    "\n",
    "dictionary = corpora.Dictionary(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
