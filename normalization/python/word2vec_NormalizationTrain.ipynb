{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoldFire\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:1209: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "import os\n",
    "import codecs\n",
    "import pandas as pd \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models...\n"
     ]
    }
   ],
   "source": [
    "print('Loading models...')\n",
    "model = gensim.models.Word2Vec.load('./models/idwiki_word2vec.model')\n",
    "modelOOV = gensim.models.Word2Vec.load('./models/modelWord2Vec_OOV.model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading training pairs...\n"
     ]
    }
   ],
   "source": [
    "print('Reading training pairs...')\n",
    "word_pairs = codecs.open('corpus_TBK-BK.csv', 'r', 'utf-8')\n",
    "\n",
    "pairs = pd.read_csv(word_pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        source      target\n",
      "0        abjat       abjad\n",
      "1           da         ada\n",
      "2           ad         ada\n",
      "3         adek        adik\n",
      "4      adpokat     advokat\n",
      "5        afdol       afdal\n",
      "6            z         aja\n",
      "7          ajk        ajak\n",
      "8        ahlak      akhlak\n",
      "9        aktip       aktif\n",
      "10   aktifitas   aktivitas\n",
      "11         ane         aku\n",
      "12          aq         aku\n",
      "13         aqu         aku\n",
      "14         eke         aku\n",
      "15        eyke         aku\n",
      "16         gua         aku\n",
      "17          ak         aku\n",
      "18           q         aku\n",
      "19        almt      alamat\n",
      "20         alm    almarhum\n",
      "21     ambeyen     ambeien\n",
      "22     ambulan    ambulans\n",
      "23   amandemen   amendemen\n",
      "24     amphibi      amfibi\n",
      "25       amvun       ampun\n",
      "26         ank        anak\n",
      "27     analisa    analisis\n",
      "28      handal       andal\n",
      "29      anggep      anggap\n",
      "..         ...         ...\n",
      "856       univ  universita\n",
      "857        tuk       untuk\n",
      "858        utk       untuk\n",
      "859     urgent       urgen\n",
      "860     ngurus        urus\n",
      "861   ngurusin        urus\n",
      "862        ust      ustadz\n",
      "863     hutang       utang\n",
      "864    varitas    varietas\n",
      "865       walo       walau\n",
      "866    walopun    walaupun\n",
      "867       wlpn    walaupun\n",
      "868        wnt      wanita\n",
      "869        wrn       warna\n",
      "870       wrna       warna\n",
      "871      wasap    whatsapp\n",
      "872      wudhu        wudu\n",
      "873        yaa          ya\n",
      "874        yap          ya\n",
      "875        yha          ya\n",
      "876        yoi          ya\n",
      "877        yup          ya\n",
      "878       yups          ya\n",
      "879          y          ya\n",
      "880         yg        yang\n",
      "881        yng        yang\n",
      "882      yutub     youtube\n",
      "883      yutup     youtube\n",
      "884      jaman       zaman\n",
      "885       zone        zona\n",
      "\n",
      "[886 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "missing = 0\n",
    "for n in range (len(pairs)):\n",
    "\tif pairs['source'][n] not in modelOOV.wv.vocab or pairs['target'][n] not in model.wv.vocab:\n",
    "\t\tmissing = missing + 1\n",
    "\t\tpairs = pairs.drop(n)\n",
    "\n",
    "pairs = pairs.reset_index(drop = True)\n",
    "print(pairs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoldFire\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\Users\\GoldFire\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating translation matrix\n",
      "display translation\n"
     ]
    }
   ],
   "source": [
    "pairs['vector_source'] = [modelOOV[pairs['source'][n]] for n in range (len(pairs))]\n",
    "pairs['vector_target'] = [model[pairs['target'][n]] for n in range (len(pairs))]\n",
    "\n",
    "# # first 5000 from both languages, to train translation matrix\n",
    "source_training_set = pairs['vector_source'][:5000]\n",
    "target_training_set = pairs['vector_target'][:5000]\n",
    "\n",
    "matrix_train_source = pd.DataFrame(source_training_set.tolist()).values\n",
    "matrix_train_target = pd.DataFrame(target_training_set.tolist()).values\n",
    "\n",
    "print('Generating translation matrix')\n",
    "\n",
    "# # Matrix W is given in  http://stackoverflow.com/questions/27980159/fit-a-linear-transformation-in-python\n",
    "translation_matrix = np.linalg.pinv(matrix_train_source).dot(matrix_train_target).T\n",
    "print ('display translation')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def most_similar_vector(self, vectenter, topn=5):\n",
    "    self.init_sims()\n",
    "    dists = np.dot(self.syn0norm, vectenter)\n",
    "    if not topn:\n",
    "        return dists\n",
    "    best = np.argsort(dists)[::-1][:topn ]\n",
    "        # ignore (don't return) words from the input\n",
    "    result = [(self.index2word[sim], float(dists[sim])) for sim in best]\n",
    "    return result[:topn]\n",
    "\n",
    "def top_translations(w,numb=5):\n",
    "    val = most_similar_vector(model.wv,translation_matrix.dot(model[w]),numb)\n",
    "    #print 'traducwithscofres ', val\n",
    "    return val\n",
    "\n",
    "\n",
    "def top_translations_list(w, numb=5):\n",
    "    val = [top_translations(w,numb)[k][0] for k in range(numb)]\n",
    "    return val\n",
    "\n",
    "def saveModelTranslationMatrix():\n",
    "    pass\n",
    "\n",
    "temp = 1\n",
    "#top_matches = [ pairs['target'][n] in top_translations_list(pairs['source'][n]) for n in range(5000,5003)] \n",
    "\n",
    "# print out source word and translation\n",
    "def display_translations():\n",
    "    for word_num in range(range_start, range_end):\n",
    "        source_word = pairs['source'][word_num]\n",
    "        translations = top_translations_list(pairs['target'][word_num]) \n",
    "        print(source_word, translations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testNormalization(word):\n",
    "    for i, elem in enumerate(pairs['source'][:]):\n",
    "        if word in elem:\n",
    "            indices = i\n",
    "            return top_translations_list(pairs['target'][indices])\n",
    "        if i == len(pairs['source'][:])-1:\n",
    "            return \"tidak ditemukan\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\GoldFire\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:12: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "  if sys.path[0] == '':\n",
      "C:\\Users\\GoldFire\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `syn0norm` (Attribute will be removed in 4.0.0, use self.wv.vectors_norm instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['sangat', 'laun', 'amat', 'paling', 'amatlah']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testNormalization(\"cewe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
